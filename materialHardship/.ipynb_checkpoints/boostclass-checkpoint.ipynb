{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "As the title indicates, this ipynb aims to import dataframes (features plus labels) and then develop functions / clases to process data, fit models, and make predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import data (just for materialHardship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import sys\n",
    "import math\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_impute_df_train_with_labels = pd.read_pickle('mean_impute_df_train_with_labels')\n",
    "median_impute_df_train_with_labels  = pd.read_pickle('median_impute_df_train_with_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_over_balanced =  pd.read_pickle('final_over_balanced_decimals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_prediction =  pd.read_pickle('df_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files from pickles...we are reading full dataframes, with columns of features attached to label columns. Function below is used to split features from class labels and make models and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into features and labels\n",
    "def split_data(df, label='materialHardship'):\n",
    "    copy = df.copy() # copy df so i dont alter original df by popping\n",
    "    y = copy.pop(label) # pop label\n",
    "    return copy, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return features and labels...feed split_data function a full data frame. This can be any from the above imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy, mh = split_data(median_impute_df_train_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test_train split. I call the test data the \"validation\" data because I think it is a better, less ambiguous term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_med, x_valid_med, y_train_med, y_valid_med = train_test_split(copy, mh, test_size=0.2) #train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rd = [1,5]\n",
    "i=1\n",
    "paramms = {\n",
    "        \"eval_metric\" : 'mae',\n",
    "        'objective' : 'reg:logistic',\n",
    "        'eta' : 1,\n",
    "        'max_depth': 1\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.147203\tvalid-mae:0.144564\n",
      "[1]\ttrain-mae:0.114837\tvalid-mae:0.114989\n",
      "[2]\ttrain-mae:0.108244\tvalid-mae:0.110495\n",
      "[3]\ttrain-mae:0.105297\tvalid-mae:0.110216\n",
      "[4]\ttrain-mae:0.102058\tvalid-mae:0.108428\n",
      "[5]\ttrain-mae:0.100458\tvalid-mae:0.107735\n",
      "[6]\ttrain-mae:0.098985\tvalid-mae:0.108918\n",
      "[7]\ttrain-mae:0.097252\tvalid-mae:0.105368\n",
      "[8]\ttrain-mae:0.095077\tvalid-mae:0.105486\n",
      "[9]\ttrain-mae:0.09456\tvalid-mae:0.106338\n",
      "[10]\ttrain-mae:0.093475\tvalid-mae:0.10722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x111841fd0>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(x_train_med,y_train_med/11) # make training matrix\n",
    "d_valid = xgb.DMatrix(x_valid_med,y_valid_med/11) # make valid matrix\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "xgb.train(params=paramms,  # train on train matrix with params\n",
    "            dtrain=d_train,\n",
    "            evals = watchlist,\n",
    "         num_boost_round=11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create XGB Classifier class. This was inspired by [this](https://www.kaggle.com/tanitter/introducing-kaggle-scripts/grid-search-xgboost-with-scikit-learn/run/23363) Kaggle script. I don't have CV functionality of SKlearn working here, though. As a result, I just wrote my own way to iterate through options to choose best model.\n",
    "\n",
    "This class is a wrapper to create a XGB object. Then, in sequence, you can a) fit a model, b) predict a model, and c) find the mean squared error score of your model on some validation / test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class XGBoostClassifier():\n",
    "    def __init__(self, **params): \n",
    "        self.clf = None \n",
    "        self.predd = None\n",
    "        self.params = params # params attribute set initially\n",
    " \n",
    "    def fit(self, x_train, y_train, x_valid, y_valid):\n",
    "        d_train = xgb.DMatrix(x_train,y_train) # make training matrix\n",
    "        d_valid = xgb.DMatrix(x_valid,y_valid) # make valid matrix\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "        self.clf = xgb.train(\n",
    "                            params=self.params['params'],  # train on train matrix with params\n",
    "                            dtrain=d_train,\n",
    "                            evals = watchlist) \n",
    "        return self.clf\n",
    " \n",
    "    def predict(self, x_test):\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "        predd = self.clf.predict(dtest)\n",
    "        return self.predd\n",
    "\n",
    "#     def score(self, x_valid, y_valid):\n",
    "#         Y = self.clf.predict(xgb.DMatrix(x_valid))\n",
    "#         return mean_squared_error(y_valid, Y)\n",
    "    \n",
    "    def get_feat_score(self):# print feature scores\n",
    "        return self.clf.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rd = [1,5]\n",
    "i=1\n",
    "paramms = {\n",
    "        \"eval_metric\" : 'rmse',\n",
    "        'objective' : 'reg:logistic',\n",
    "        'eta' : 1,\n",
    "        'max_depth': 1,\n",
    "        'num_boost_round' : num_rd[i]\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xclass = XGBoostClassifier(params = paramms)\n",
    "xclass = XGBoostClassifier(params = paramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta': 1,\n",
       " 'eval_metric': 'rmse',\n",
       " 'max_depth': 1,\n",
       " 'num_boost_round': 5,\n",
       " 'objective': 'reg:logistic'}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'eta': 1,\n",
       "  'eval_metric': 'rmse',\n",
       "  'max_depth': 1,\n",
       "  'num_boost_round': 5,\n",
       "  'objective': 'reg:logistic'}}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xclass.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.165802\tvalid-rmse:0.165117\n",
      "[1]\ttrain-rmse:0.14707\tvalid-rmse:0.149667\n",
      "[2]\ttrain-rmse:0.143777\tvalid-rmse:0.145141\n",
      "[3]\ttrain-rmse:0.141702\tvalid-rmse:0.145921\n",
      "[4]\ttrain-rmse:0.140933\tvalid-rmse:0.145243\n",
      "[5]\ttrain-rmse:0.140124\tvalid-rmse:0.144267\n",
      "[6]\ttrain-rmse:0.138483\tvalid-rmse:0.146228\n",
      "[7]\ttrain-rmse:0.136432\tvalid-rmse:0.141445\n",
      "[8]\ttrain-rmse:0.134645\tvalid-rmse:0.142783\n",
      "[9]\ttrain-rmse:0.13329\tvalid-rmse:0.142738\n"
     ]
    }
   ],
   "source": [
    "fitt = xclass.fit(x_train_med, \n",
    "                  y_train_med/11,\n",
    "                  x_valid_med, \n",
    "                  y_valid_med/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x114aea7f0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3254022 ,  0.18412694,  0.21786654, ...,  0.24428976,\n",
       "        0.29976332,  0.08016646], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitt.predict(xgb.DMatrix(df_prediction_good_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = xclass.get_feat_score()\n",
    "\n",
    "def get_feature_df(d):\n",
    "    '''given xgboost feature score object\n",
    "    put into pretty df'''\n",
    "    feature = []\n",
    "    f_importance = []\n",
    "    for w in sorted(d, key=d.get, reverse=True):\n",
    "        feature.append(w)\n",
    "        f_importance.append(d[w])\n",
    "\n",
    "    feature_import_df = pd.DataFrame(\n",
    "        {'feature': feature,\n",
    "         'feature importance': f_importance,\n",
    "        })\n",
    "    return feature_import_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>feature importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>challengeID</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m1citywt</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m1lenmin</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cm1bsex</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m1a15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature  feature importance\n",
       "0  challengeID                  47\n",
       "1     m1citywt                  21\n",
       "2     m1lenmin                  11\n",
       "3      cm1bsex                   4\n",
       "4        m1a15                   4"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_df(d).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prediction_good_cols = df_prediction[x_train_med.columns.values] # keep the columns that are in training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predy = fitt.predict(xgb.DMatrix(df_prediction_good_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_preds = fitt.predict(xgb.DMatrix(x_train_med))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3254022 ,  0.18412694,  0.21786654, ...,  0.24428976,\n",
       "        0.29976332,  0.08016646], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07566336,  0.04096115,  0.41326064, ...,  0.03218818,\n",
       "        0.27110511,  0.04082593], dtype=float32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# errs = xclass.score(x_train_med, y_train_med/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027180740026536234"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7004435820937411"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train_med, train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search cv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rd = [1,20]\n",
    "errors = []\n",
    "for i in range(len(depths)):\n",
    "    paramms = {\n",
    "        \"eval_metric\" : 'rmse',\n",
    "        'objective' : 'reg:logistic',\n",
    "        'eta' : 0.02,\n",
    "        'max_depth': 20,\n",
    "        'num_boost_round' : num_rd[i]\n",
    "       }\n",
    "    xclass = XGBoostClassifier(params= paramms)\n",
    "    fitt = xclass.fit(x_train_med, \n",
    "                  y_train_med/11,\n",
    "                  x_valid_med, \n",
    "                  y_valid_med/11)\n",
    "    predy = xclass.predict(df_prediction_good_cols)\n",
    "    errs = xclass.score(x_train_med, y_train_med/11)\n",
    "    errors.append(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0027180740026536234, 0.0027180740026536234]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramms = {\"eval_metric\" : 'rmse',\n",
    "        'num_class' : 2,\n",
    "        'eta' : 0.1,\n",
    "        'num_boost_round' : 8,\n",
    "        'max_depth' : 2,\n",
    "        'subsample' : 0.5,\n",
    "        'colsample_bytree' : 1.0,\n",
    "        'objective': 'reg:logistic',\n",
    "        'evals' :[ (xgb.DMatrix(x_train_med), 'train'),  (xgb.DMatrix(x_valid_med), 'valid')],\n",
    "        'verbose_eval':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# focus here!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting b/c valid error > train error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() takes from 2 to 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d4b817eda048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mCV_rfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mCV_rfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_med\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_med\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid_med\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_med\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m# print(CV_rfc.best_params_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() takes from 2 to 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Build a classification task using 3 informative features\n",
    "# X, y = make_classification(n_samples=1000,\n",
    "#                            n_features=10,\n",
    "#                            n_informative=3,\n",
    "#                            n_redundant=0,\n",
    "#                            n_repeated=0,\n",
    "#                            n_classes=2,\n",
    "#                            random_state=0,\n",
    "#                            shuffle=False)\n",
    "\n",
    "\n",
    "# rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "\n",
    "param_grid = { \n",
    "    'num_boost_round': [10, 25, 50]\n",
    "}\n",
    "# d_train = xgb.DMatrix(x_train_med,y_train_med)\n",
    "\n",
    "CV_rfc = GridSearchCV(xclass, param_grid=param_grid, cv= 2)\n",
    "CV_rfc.fit(x_train_med,y_train_med/11)\n",
    "# print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "b'[08:17:43] src/metric/elementwise_metric.cc:28: Check failed: preds.size() == info.labels.size() (2920 vs. 292) label and prediction size not match, hint: use merror or mlogloss for multi-class classification\\n\\nStack trace returned 6 entries:\\n[bt] (0) 0   libxgboost.so                       0x000000010f2811d5 _ZN4dmlc15LogMessageFatalD2Ev + 37\\n[bt] (1) 1   libxgboost.so                       0x000000010f2c8f73 _ZNK7xgboost6metric13EvalEWiseBaseINS0_8EvalRMSEEE4EvalERKNSt3__16vectorIfNS4_9allocatorIfEEEERKNS_8MetaInfoEb + 995\\n[bt] (2) 2   libxgboost.so                       0x000000010f27e319 _ZN7xgboost11LearnerImpl11EvalOneIterEiRKNSt3__16vectorIPNS_7DMatrixENS1_9allocatorIS4_EEEERKNS2_INS1_12basic_stringIcNS1_11char_traitsIcEENS5_IcEEEENS5_ISE_EEEE + 889\\n[bt] (3) 3   libxgboost.so                       0x000000010f293884 XGBoosterEvalOneIter + 612\\n[bt] (4) 4   _ctypes.so                          0x0000000103918127 ffi_call_unix64 + 79\\n[bt] (5) 5   python                              0x00007fff5bffbe10 __progname + 140730441903472\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-67af4b94cf6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'binary:logistic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m clf.fit(x_train_med,y_train_med/11,  early_stopping_rounds=10, eval_metric=\"rmse\", \n\u001b[0;32m----> 3\u001b[0;31m     eval_set=[(x_valid_med, y_valid_med/11)])\n\u001b[0m",
      "\u001b[0;32m/Users/wnowak/Downloads/xgboost-0.6a2/xgboost/xgboost/python-package/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    462\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wnowak/Downloads/xgboost-0.6a2/xgboost/xgboost/python-package/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wnowak/Downloads/xgboost-0.6a2/xgboost/xgboost/python-package/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# check evaluation result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mbst_eval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst_eval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst_eval_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wnowak/Downloads/xgboost-0.6a2/xgboost/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36meval_set\u001b[0;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[1;32m    877\u001b[0m         _check_call(_LIB.XGBoosterEvalOneIter(self.handle, iteration,\n\u001b[1;32m    878\u001b[0m                                               \u001b[0mdmats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                                               ctypes.byref(msg)))\n\u001b[0m\u001b[1;32m    880\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wnowak/Downloads/xgboost-0.6a2/xgboost/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[08:17:43] src/metric/elementwise_metric.cc:28: Check failed: preds.size() == info.labels.size() (2920 vs. 292) label and prediction size not match, hint: use merror or mlogloss for multi-class classification\\n\\nStack trace returned 6 entries:\\n[bt] (0) 0   libxgboost.so                       0x000000010f2811d5 _ZN4dmlc15LogMessageFatalD2Ev + 37\\n[bt] (1) 1   libxgboost.so                       0x000000010f2c8f73 _ZNK7xgboost6metric13EvalEWiseBaseINS0_8EvalRMSEEE4EvalERKNSt3__16vectorIfNS4_9allocatorIfEEEERKNS_8MetaInfoEb + 995\\n[bt] (2) 2   libxgboost.so                       0x000000010f27e319 _ZN7xgboost11LearnerImpl11EvalOneIterEiRKNSt3__16vectorIPNS_7DMatrixENS1_9allocatorIS4_EEEERKNS2_INS1_12basic_stringIcNS1_11char_traitsIcEENS5_IcEEEENS5_ISE_EEEE + 889\\n[bt] (3) 3   libxgboost.so                       0x000000010f293884 XGBoosterEvalOneIter + 612\\n[bt] (4) 4   _ctypes.so                          0x0000000103918127 ffi_call_unix64 + 79\\n[bt] (5) 5   python                              0x00007fff5bffbe10 __progname + 140730441903472\\n'"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(n_estimators = 100, objective= 'binary:logistic',)\n",
    "clf.fit(x_train_med,y_train_med/11,  early_stopping_rounds=10, eval_metric=\"rmse\", \n",
    "    eval_set=[(x_valid_med, y_valid_med/11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-184-431e3546a930>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m'colsample_bytree'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     }\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_med\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_med\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'params'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x12007f7f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wnowak/Downloads/xgboost-0.6a2/xgboost/xgboost/python-package/xgboost/core.py\", line 337, in __del__\n",
      "    _check_call(_LIB.XGDMatrixFree(self.handle))\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n",
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x1200570f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wnowak/Downloads/xgboost-0.6a2/xgboost/xgboost/python-package/xgboost/core.py\", line 337, in __del__\n",
      "    _check_call(_LIB.XGDMatrixFree(self.handle))\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4426950408889634\n",
      "colsample_bytree: 0.9\n",
      "eta: 0.05\n",
      "max_depth: 6\n",
      "num_boost_round: 100\n",
      "subsample: 0.9\n",
      "['b']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    " \n",
    "import sys\n",
    "import math\n",
    " \n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    " \n",
    "# sys.path.append('xgboost/wrapper/')\n",
    "import xgboost as xgb\n",
    " \n",
    " \n",
    "class XGBoostClassifier():\n",
    "    def __init__(self, num_boost_round=10, **params):\n",
    "        self.clf = None\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.params = params\n",
    "        self.params.update({'objective': 'multi:softprob'})\n",
    " \n",
    "    def fit(self, X, y, num_boost_round=None):\n",
    "        num_boost_round = num_boost_round or self.num_boost_round\n",
    "        self.label2num = dict((label, i) for i, label in enumerate(sorted(set(y))))\n",
    "        dtrain = xgb.DMatrix(X, label=[self.label2num[label] for label in y])\n",
    "        self.clf = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=num_boost_round)\n",
    " \n",
    "    def predict(self, X):\n",
    "        num2label = dict((i, label)for label, i in self.label2num.items())\n",
    "        Y = self.predict_proba(X)\n",
    "        y = np.argmax(Y, axis=1)\n",
    "        return np.array([num2label[i] for i in y])\n",
    " \n",
    "    def predict_proba(self, X):\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        return self.clf.predict(dtest)\n",
    " \n",
    "    def score(self, X, y):\n",
    "        Y = self.predict_proba(X)\n",
    "        return 1 / logloss(y, Y)\n",
    " \n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    " \n",
    "    def set_params(self, **params):\n",
    "        if 'num_boost_round' in params:\n",
    "            self.num_boost_round = params.pop('num_boost_round')\n",
    "        if 'objective' in params:\n",
    "            del params['objective']\n",
    "        self.params.update(params)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "def logloss(y_true, Y_pred):\n",
    "    label2num = dict((name, i) for i, name in enumerate(sorted(set(y_true))))\n",
    "    return -1 * sum(math.log(y[label2num[label]]) if y[label2num[label]] > 0 else -np.inf for y, label in zip(Y_pred, y_true)) / len(Y_pred)\n",
    "\n",
    "\n",
    "def main():\n",
    "    clf = XGBoostClassifier(\n",
    "        eval_metric = 'auc',\n",
    "        num_class = 2,\n",
    "        nthread = 4,\n",
    "        eta = 0.1,\n",
    "        num_boost_round = 80,\n",
    "        max_depth = 12,\n",
    "        subsample = 0.5,\n",
    "        colsample_bytree = 1.0,\n",
    "        silent = 1,\n",
    "        )\n",
    "    parameters = {\n",
    "        'num_boost_round': [100, 250, 500],\n",
    "        'eta': [0.05, 0.1, 0.3],\n",
    "        'max_depth': [6, 9, 12],\n",
    "        'subsample': [0.9, 1.0],\n",
    "        'colsample_bytree': [0.9, 1.0],\n",
    "    }\n",
    "    clf = GridSearchCV(clf, parameters, n_jobs=1, cv=2)\n",
    "    \n",
    "    clf.fit([[1,2], [3,4], [2,1], [4,3]], ['a', 'b', 'a', 'b'])\n",
    "    best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])\n",
    "    print(score)\n",
    "    for param_name in sorted(best_parameters.keys()):\n",
    "        print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                \n",
    "    print(clf.predict([[1,2]]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create submission\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>challengeID</th>\n",
       "      <th>materialHardship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.228553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.208423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.192934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.217176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.100321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.186167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.210725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.084569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.121868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.103562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    challengeID  materialHardship\n",
       "0             1          0.228553\n",
       "1             2          0.208423\n",
       "2             3          0.192934\n",
       "3             4          0.217176\n",
       "4             5          0.100321\n",
       "7             8          0.186167\n",
       "10           11          0.210725\n",
       "11           12          0.084569\n",
       "14           15          0.121868\n",
       "16           17          0.103562"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to add train labels to submission too, I think\n",
    "print('create submission')\n",
    "preds = pd.DataFrame()\n",
    "preds['challengeID'] = df_prediction['challengeID']\n",
    "preds['materialHardship'] = p_test\n",
    "preds.sort_values(by='challengeID').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tack on given train values\n",
    "sub = preds.append(non_na_y_train_materialHardship)\n",
    "sub.sort_values(by='challengeID').head(15)\n",
    "\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, we send in a submission that predicts all missing values (both missing MH values from train set and also values for IDs that were never in train to start). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
