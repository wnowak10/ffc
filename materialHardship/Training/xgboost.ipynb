{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare data (just for materialHardship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from missing_values import final_over_balanced, df_train, median_impute_df_train, median_impute_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_impute_df_train, median_impute_df_train, final_over_balanced = pd.read_pickle('mean_impute_df_train'), pd.read_pickle('median_impute_df_train'), pd.read_pickle('final_over_balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_na_y_train_materialHardship =  pd.read_pickle('non_na_y_train_materialHardship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_prediction =  pd.read_pickle('df_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5668, 11986)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_over_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See missing_values notebook for full explanation of the preceding dataframes. We'll do some testing here to see which seems to work best on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation and training data ready\n"
     ]
    }
   ],
   "source": [
    "# Create validation set \n",
    "x_train, x_valid, y_train, y_valid = train_test_split(median_impute_df_train, non_na_y_train_materialHardship['materialHardship'], test_size=0.2)\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "print('validation and training data ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prediction_good_cols = df_prediction[x_train.columns.values] # keep the columns that are in training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'reg:logistic'\n",
    "params['eval_metric'] = 'rmse'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 20\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train xgb\n",
      "[0]\ttrain-rmse:0.418161\tvalid-rmse:0.417785\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 50 rounds.\n",
      "[4]\ttrain-rmse:0.388627\tvalid-rmse:0.391049\n"
     ]
    }
   ],
   "source": [
    "# XGB train\n",
    "# model = xgb.XGBClassifier()\n",
    "print('train xgb')\n",
    "# bst = model.fit(df_test_mh, y_train_mh)\n",
    "bst = xgb.train(params, d_train, 5, watchlist, early_stopping_rounds=50, verbose_eval=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>feature importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m4i23o</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m5f23e</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m5f23l</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hv3pvtwceil_m</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p5q3bo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m4i16a_4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>m5g18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hv3j16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m5f23a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>m5b91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  feature importance\n",
       "0         m4i23o                   5\n",
       "1         m5f23e                   4\n",
       "2         m5f23l                   4\n",
       "3  hv3pvtwceil_m                   3\n",
       "4         p5q3bo                   3\n",
       "5       m4i16a_4                   3\n",
       "6          m5g18                   3\n",
       "7         hv3j16                   3\n",
       "8         m5f23a                   3\n",
       "9          m5b91                   3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature importances df\n",
    "d = bst.get_fscore()  # print feature scores\n",
    "feature = []\n",
    "f_importance = []\n",
    "for w in sorted(d, key=d.get, reverse=True):\n",
    "    feature.append(w)\n",
    "    f_importance.append(d[w])\n",
    "\n",
    "feature_import_df = pd.DataFrame(\n",
    "    {'feature': feature,\n",
    "     'feature importance': f_importance,\n",
    "    })\n",
    "\n",
    "feature_import_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('predict with xgb')\n",
    "d_test = xgb.DMatrix(df_prediction_good_cols)\n",
    "# d_test\n",
    "p_test = bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.479083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.479083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.479083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.471880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.470390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.479083\n",
       "1  0.479083\n",
       "2  0.479083\n",
       "3  0.471880\n",
       "4  0.470390"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(p_test).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above works to train a simple model. But I want to see if I can use SKlearn to do some hyper-parameter grid search. I found a cool kaggle [kernel](https://www.kaggle.com/tanitter/introducing-kaggle-scripts/grid-search-xgboost-with-scikit-learn/run/23363) that might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO...work through\n",
    "class XGBoostClassifier():\n",
    "    def __init__(self, num_boost_round=10, **params):\n",
    "        self.clf = None\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.params = params\n",
    "        self.params.update({'objective': 'multi:softprob'})\n",
    " \n",
    "    def fit(self, X, y, num_boost_round=None):\n",
    "        num_boost_round = num_boost_round or self.num_boost_round\n",
    "        self.label2num = dict((label, i) for i, label in enumerate(sorted(set(y))))\n",
    "        dtrain = xgb.DMatrix(X, label=[self.label2num[label] for label in y])\n",
    "        self.clf = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=num_boost_round)\n",
    " \n",
    "    def predict(self, X):\n",
    "        num2label = dict((i, label)for label, i in self.label2num.items())\n",
    "        Y = self.predict_proba(X)\n",
    "        y = np.argmax(Y, axis=1)\n",
    "        return np.array([num2label[i] for i in y])\n",
    " \n",
    "    def predict_proba(self, X):\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        return self.clf.predict(dtest)\n",
    " \n",
    "    def score(self, X, y):\n",
    "        Y = self.predict_proba(X)\n",
    "        return 1 / logloss(y, Y)\n",
    " \n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    " \n",
    "    def set_params(self, **params):\n",
    "        if 'num_boost_round' in params:\n",
    "            self.num_boost_round = params.pop('num_boost_round')\n",
    "        if 'objective' in params:\n",
    "            del params['objective']\n",
    "        self.params.update(params)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "def logloss(y_true, Y_pred):\n",
    "    label2num = dict((name, i) for i, name in enumerate(sorted(set(y_true))))\n",
    "    return -1 * sum(math.log(y[label2num[label]]) if y[label2num[label]] > 0 else -np.inf for y, label in zip(Y_pred, y_true)) / len(Y_pred)\n",
    "\n",
    "\n",
    "def main():\n",
    "    clf = XGBoostClassifier(\n",
    "        eval_metric = 'auc',\n",
    "        num_class = 2,\n",
    "        nthread = 4,\n",
    "        eta = 0.1,\n",
    "        num_boost_round = 80,\n",
    "        max_depth = 12,\n",
    "        subsample = 0.5,\n",
    "        colsample_bytree = 1.0,\n",
    "        silent = 1,\n",
    "        )\n",
    "    parameters = {\n",
    "        'num_boost_round': [100, 250, 500],\n",
    "        'eta': [0.05, 0.1, 0.3],\n",
    "        'max_depth': [6, 9, 12],\n",
    "        'subsample': [0.9, 1.0],\n",
    "        'colsample_bytree': [0.9, 1.0],\n",
    "    }\n",
    "    clf = GridSearchCV(clf, parameters, n_jobs=1, cv=2)\n",
    "    \n",
    "    clf.fit([[1,2], [3,4], [2,1], [4,3]], ['a', 'b', 'a', 'b'])\n",
    "    best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])\n",
    "    print(score)\n",
    "    for param_name in sorted(best_parameters.keys()):\n",
    "        print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                \n",
    "    print(clf.predict([[1,2]]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to add train labels to submission too, I think\n",
    "print('create submission')\n",
    "preds = pd.DataFrame()\n",
    "preds['challengeID'] = df_prediction['challengeID']\n",
    "preds['materialHardship'] = p_test\n",
    "preds.sort_values(by='challengeID').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tack on given train values\n",
    "sub = preds.append(non_na_y_train_materialHardship)\n",
    "sub.sort_values(by='challengeID').head(15)\n",
    "\n",
    "# sub.to_csv('simple_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, we send in a submission that predicts all missing values (both missing MH values from train set and also values for IDs that were never in train to start). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
